{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Parameterize EVT (must first instantiate EVT pckg)\n",
    "- Parameterize Reach 1 inflow\n",
    "- Parameterize Lumprem models\n",
    "- Covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import flopy\n",
    "from flopy.utils.gridintersect import GridIntersect\n",
    "import pyemu\n",
    "\n",
    "from shapely.geometry import Polygon, Point\n",
    "import shapefile\n",
    "from shapely.prepared import prep\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "plt.rcParams['font.size']=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_pth = os.path.join('..', 'bins', 'win') if 'nt' in os.name else os.path.join('..', 'bins', 'linux') # Binaries\n",
    "shapefile_pth = os.path.join('..', 'data', 'raw_data', 'shapefiles')\n",
    "observations_pth = os.path.join('..', 'data', 'observations') # Measured data (field obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ghbpp.dat',\n",
       " 'ghbpp.tpl',\n",
       " 'ghb_new.ts',\n",
       " 'hagfors_1.cbb',\n",
       " 'hagfors_1.disv',\n",
       " 'hagfors_1.disv.grb',\n",
       " 'hagfors_1.disv_botm.txt',\n",
       " 'hagfors_1.disv_cell2d.txt',\n",
       " 'hagfors_1.disv_top.txt',\n",
       " 'hagfors_1.disv_vertices.txt',\n",
       " 'hagfors_1.ghb',\n",
       " 'hagfors_1.hds',\n",
       " 'hagfors_1.ic',\n",
       " 'hagfors_1.ic_strt.txt',\n",
       " 'hagfors_1.ims',\n",
       " 'hagfors_1.lst',\n",
       " 'hagfors_1.nam',\n",
       " 'hagfors_1.npf',\n",
       " 'hagfors_1.npf_icelltype.txt',\n",
       " 'hagfors_1.npf_k.txt',\n",
       " 'hagfors_1.npf_k33.txt',\n",
       " 'hagfors_1.obs',\n",
       " 'hagfors_1.oc',\n",
       " 'hagfors_1.rch',\n",
       " 'hagfors_1.sfr',\n",
       " 'hagfors_1.sfr.bud',\n",
       " 'hagfors_1.sfr.obs',\n",
       " 'hagfors_1.sto',\n",
       " 'hagfors_1.sto_iconvert.txt',\n",
       " 'hagfors_1.sto_ss.txt',\n",
       " 'hagfors_1.sto_sy.txt',\n",
       " 'hagfors_1.tdis',\n",
       " 'head.obs.csv',\n",
       " 'lr_blue.tpl',\n",
       " 'lr_green.tpl',\n",
       " 'lr_lu1.tpl',\n",
       " 'lr_red.tpl',\n",
       " 'lr_yellow.tpl',\n",
       " 'mfsim.lst',\n",
       " 'mfsim.nam',\n",
       " 'rch_new.ts',\n",
       " 'sfr_gage.obs.csv',\n",
       " 'sfr_pred_leak.obs.csv',\n",
       " 'sfr_reach1_inflow.ts',\n",
       " 'sfr_reach_evaporation.ts',\n",
       " 'sfr_reach_rainfall.ts',\n",
       " 'sfr_rtp_initial.csv',\n",
       " 'sfr_segfile.dat',\n",
       " 'sfr_stage.obs.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model_ws = os.path.join('..', 'temp_flopy_lumprem')\n",
    "os.listdir(org_model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ghbpp.dat',\n",
       " 'ghbpp.tpl',\n",
       " 'ghb_new.ts',\n",
       " 'hagfors_1.cbb',\n",
       " 'hagfors_1.disv',\n",
       " 'hagfors_1.disv.grb',\n",
       " 'hagfors_1.disv_botm.txt',\n",
       " 'hagfors_1.disv_cell2d.txt',\n",
       " 'hagfors_1.disv_top.txt',\n",
       " 'hagfors_1.disv_vertices.txt',\n",
       " 'hagfors_1.ghb',\n",
       " 'hagfors_1.hds',\n",
       " 'hagfors_1.ic',\n",
       " 'hagfors_1.ic_strt.txt',\n",
       " 'hagfors_1.ims',\n",
       " 'hagfors_1.lst',\n",
       " 'hagfors_1.nam',\n",
       " 'hagfors_1.npf',\n",
       " 'hagfors_1.npf_icelltype.txt',\n",
       " 'hagfors_1.npf_k.txt',\n",
       " 'hagfors_1.npf_k33.txt',\n",
       " 'hagfors_1.obs',\n",
       " 'hagfors_1.oc',\n",
       " 'hagfors_1.rch',\n",
       " 'hagfors_1.sfr',\n",
       " 'hagfors_1.sfr.bud',\n",
       " 'hagfors_1.sfr.obs',\n",
       " 'hagfors_1.sto',\n",
       " 'hagfors_1.sto_iconvert.txt',\n",
       " 'hagfors_1.sto_ss.txt',\n",
       " 'hagfors_1.sto_sy.txt',\n",
       " 'hagfors_1.tdis',\n",
       " 'head.obs.csv',\n",
       " 'lr_blue.tpl',\n",
       " 'lr_green.tpl',\n",
       " 'lr_lu1.tpl',\n",
       " 'lr_red.tpl',\n",
       " 'lr_yellow.tpl',\n",
       " 'mfsim.lst',\n",
       " 'mfsim.nam',\n",
       " 'rch_new.ts',\n",
       " 'sfr_gage.obs.csv',\n",
       " 'sfr_pred_leak.obs.csv',\n",
       " 'sfr_reach1_inflow.ts',\n",
       " 'sfr_reach_evaporation.ts',\n",
       " 'sfr_reach_rainfall.ts',\n",
       " 'sfr_rtp_initial.csv',\n",
       " 'sfr_segfile.dat',\n",
       " 'sfr_stage.obs.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model_ws = os.path.join('..', 'temp_pst_from')\n",
    "if os.path.exists(tmp_model_ws):\n",
    "    shutil.rmtree(tmp_model_ws)\n",
    "shutil.copytree(org_model_ws,tmp_model_ws)\n",
    "os.listdir(tmp_model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_name = 'hagfors_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-02 17:03:51.839668 starting: opening PstFrom.log for logging\n",
      "2022-02-02 17:03:51.842656 starting PstFrom process\n",
      "2022-02-02 17:03:51.843654 starting: setting up dirs\n",
      "2022-02-02 17:03:51.844655 starting: copying original_d '..\\temp_pst_from' to new_d '..\\hagfors_1_template'\n",
      "2022-02-02 17:03:51.978292 finished: copying original_d '..\\temp_pst_from' to new_d '..\\hagfors_1_template' took: 0:00:00.133637\n",
      "2022-02-02 17:03:51.979620 finished: setting up dirs took: 0:00:00.135966\n"
     ]
    }
   ],
   "source": [
    "template_ws = os.path.join('..', f'{ml_name}_template')\n",
    "\n",
    "pf = pyemu.utils.PstFrom(\n",
    "    original_d=tmp_model_ws,\n",
    "    new_d=template_ws,\n",
    "    remove_existing=True,\n",
    "    longnames=True,\n",
    "    spatial_reference=None,\n",
    "    zero_based=False,\n",
    "    tpl_subfolder='templates',\n",
    "    start_datetime='1-1-2016'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading simulation...\n",
      "  loading simulation name file...\n",
      "  loading tdis package...\n",
      "  loading model gwf6...\n",
      "    loading package disv...\n",
      "WARNING: Unable to resolve dimension of ('gwf6', 'disv', 'cell2d', 'cell2d', 'icvert') based on shape \"ncvert\".\n",
      "    loading package rch...\n",
      "    loading package sto...\n",
      "    loading package oc...\n",
      "    loading package npf...\n",
      "    loading package ic...\n",
      "    loading package sfr...\n",
      "    loading package obs...\n",
      "    loading package ghb...\n",
      "  loading ims package hagfors_1...\n"
     ]
    }
   ],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(ml_name, 'mf6', os.path.join(bins_pth, 'mf6'), template_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy template files into .tpl folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(old_pth, new_pth, file):\n",
    "    from_ = os.path.join(old_pth, file)\n",
    "    to_ = os.path.join(new_pth, file)\n",
    "    shutil.move(from_, to_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(tmp_model_ws):\n",
    "    if '.tpl' in filename:\n",
    "        shutil.move(os.path.join(template_ws, filename), os.path.join(template_ws, 'templates', filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Add parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all data external so that PEST can adjust parameter values during history matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.set_all_data_external(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.write_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwf = sim.get_model(ml_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "plt.ticklabel_format(axis='both', style='plain', useOffset=False) #Show coordinates\n",
    "ax.set_title('Model grid', fontsize=18)\n",
    "\n",
    "mapview = flopy.plot.PlotMapView(gwf, layer=0)\n",
    "linecollection = mapview.plot_grid(lw=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate pilot points for kh, kv, sy and ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import (head, stage & flow) measurement locations, so that we can see how the pilot points will be located in relation to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_obs = pd.read_excel(os.path.join(observations_pth, 'obs_flow_and_stage.xlsx')) # contains both stage & flow\n",
    "head_obs = pd.read_excel(os.path.join(observations_pth, 'obs_head_per_layer.xlsx'))#.drop_duplicates(subset=['POINT_X', 'POINT_Y'])\n",
    "head_obs['TYPE'] = 'HEAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_points = pd.concat([head_obs, sfr_obs])\n",
    "measurement_points['NAME'] = measurement_points['NAME'].str.lower()\n",
    "measurement_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas = pyemu.utils.smp_to_dataframe(os.path.join('..', 'data', 'olproc_input', 'obs-head-all.ssf'))\n",
    "stg_meas = pyemu.utils.smp_to_dataframe(os.path.join('..', 'data', 'olproc_input', 'obs-stage1.ssf'))\n",
    "inf_meas = pyemu.utils.smp_to_dataframe(os.path.join('..', 'data', 'olproc_input', 'obs-gage1.ssf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [hds_meas, stg_meas, inf_meas]:\n",
    "    df['name'] = df['name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_points = measurement_points.loc[measurement_points['NAME'].isin(pd.concat([hds_meas, stg_meas, inf_meas]).name.unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate pilot points using shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_boundary = shapefile.Reader(os.path.join(shapefile_pth, 'model_boundary.shp')) # Model boundary shapefile\n",
    "mlb_shape = np.array(np.rint(ml_boundary.shapeRecords()[0].shape.points)) # Model boundary array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a shapely polygon of the model boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_shapely = Polygon(mlb_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prepared polygon of the model boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_shapely_prep = prep(mlb_shapely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a rectangular mesh of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, ymin, ymax = 426900, 427700, 6654650, 6655350 # These are the same coordinates used to construct the base-grid\n",
    "resolution = 28 # Equal space (in meters) between pilot points\n",
    "basepoints = []\n",
    "for lat in np.arange(xmin, xmax, resolution):\n",
    "    for lon in np.arange(ymin, ymax, resolution):\n",
    "        basepoints.append(Point((round(lat,4), round(lon,4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the shapely `contains` (point-in-polygon method) to select points inside the model boundary (increase the number of pps once workflow is confirmed working):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepip = [] # Basepoints in polygon\n",
    "for point in basepoints:\n",
    "    if mlb_shapely_prep.contains(point):\n",
    "        basepip.append(point)\n",
    "print(f'Number of points per layer: {len(basepip)}') # We need to extend it into three dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "plt.ticklabel_format(axis='both', style='plain', useOffset=False) #Show coordinates\n",
    "ax.set_title('Coarse pilot point locations', fontsize=18)\n",
    "\n",
    "mapview = flopy.plot.PlotMapView(gwf, layer=0)\n",
    "linecollection = mapview.plot_grid(lw=0.25)\n",
    "\n",
    "x = np.array([i.coords[0] for i in basepip])[:,0]\n",
    "y = np.array([i.coords[0] for i in basepip])[:,1]\n",
    "plt.scatter(x, y, s=5, c='black', alpha=0.5, label='PILOT POINT')\n",
    "\n",
    "for category in measurement_points['TYPE'].unique():\n",
    "    x = measurement_points.loc[measurement_points['TYPE'] == category]['POINT_X']\n",
    "    y = measurement_points.loc[measurement_points['TYPE'] == category]['POINT_Y']\n",
    "    ax.scatter(x, y, s=20, alpha=0.6, label=category)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These points now need to be assigned a z-value, since we are going to use 3D-pilot points. To do this, we will need to intersect the model grid and retrieve the z-values of each layer, so that a copy of these pps can be positioned in each of the three layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = GridIntersect(gwf.modelgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect = {\n",
    "    'cellids': [],\n",
    "    'vertices': [],\n",
    "    'ixshapes': [],\n",
    "}\n",
    "for point in basepip:\n",
    "    pp_intersect['cellids'].append(ix.intersect(point).cellids[0])\n",
    "    pp_intersect['vertices'].append(ix.intersect(point).vertices[0])\n",
    "    pp_intersect['ixshapes'].append(ix.intersect(point).ixshapes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect = pd.DataFrame(pp_intersect)\n",
    "display(pp_intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_elevation(elevation_array, index_array):\n",
    "    '''\n",
    "    returns list of elevations\n",
    "    '''\n",
    "    \n",
    "    elevations = list(elevation_array)\n",
    "    indices = list(index_array)\n",
    "    \n",
    "    return [elevation_array[i] for i in index_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['top'] = get_grid_elevation(elevation_array = gwf.modelgrid.top, index_array=pp_intersect['cellids'])\n",
    "pp_intersect['botm_1'] = get_grid_elevation(gwf.modelgrid.botm[0], pp_intersect['cellids'])\n",
    "pp_intersect['botm_2'] = get_grid_elevation(gwf.modelgrid.botm[1], pp_intersect['cellids'])\n",
    "pp_intersect['botm_3'] = get_grid_elevation(gwf.modelgrid.botm[2], pp_intersect['cellids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pps in-between the layer boundaries (i.e. vertically centered in the cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['pps_l1'] = (pp_intersect['top'] + pp_intersect['botm_1']) / 2\n",
    "pp_intersect['pps_l2'] = (pp_intersect['botm_1'] + pp_intersect['botm_2']) / 2\n",
    "pp_intersect['pps_l3'] = (pp_intersect['botm_2'] + pp_intersect['botm_3']) / 2\n",
    "display(pp_intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['x'] = np.array([i.coords[0] for i in basepip])[:,0]\n",
    "pp_intersect['y'] = np.array([i.coords[0] for i in basepip])[:,1]\n",
    "pp_intersect = pp_intersect[['x', 'y', 'pps_l1', 'pps_l2', 'pps_l3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataframe into a 3d pilot point file format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d = pd.melt(pp_intersect, id_vars=['x', 'y'], value_vars=['pps_l1', 'pps_l2', 'pps_l3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "\n",
    "fig = pyplot.figure(figsize=(6,6))\n",
    "ax = Axes3D(fig)\n",
    "ax.set_box_aspect([2,2,1])\n",
    "\n",
    "x, y = pp_coarse3d.x.values, pp_coarse3d.y.values\n",
    "\n",
    "ax.scatter(x, y, pp_coarse3d.value.values)\n",
    "plt.title('pps in 3d-space')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d['name'] = [f'ppc{i:04d}' for i in pp_coarse3d.index.values] # ppc = pilot point coarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d[['zone','val']] = 1, 1 #redundant?\n",
    "pp_coarse3d['z'] = pp_coarse3d['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d['layer'] = [int(i[-1]) for i in pp_coarse3d['variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d = pp_coarse3d[['name', 'x', 'y', 'z', 'zone', 'val', 'layer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pp_coarse3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumpyrem import run\n",
    "#MKPPSTAT\n",
    "# mkppstat requires no headers in ppoint file ...sigh...\n",
    "pp_coarse3d.to_csv(os.path.join(template_ws, 'mkppoints3d_coarse.dat'),\n",
    "                       header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for MKPPSTAT\n",
    "# A (a)-factor of 1.5 is often reasonable. from tutorial \n",
    "npoints_h, npoints_v = 10, 10 # np-horizontal, np-vertical\n",
    "a_h, a_v = 1.2, 1.2 # a-horizontal, a-vertical\n",
    "# run MKPPSTAT\n",
    "run.run_process(\n",
    "    'mkppstat3d',\n",
    "    path=template_ws,\n",
    "    commands=['mkppoints3d_coarse.dat', npoints_h, a_h, npoints_v, a_v, 'ppstat3d_coarse.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PPCOV3D_SVA - pilot point covariance 3d - spatially varying anisotropy\n",
    "run.run_process(\n",
    "    'ppcov3d_sva',\n",
    "    path=template_ws,\n",
    "    commands=['ppstat3d_coarse.dat', 'y', 1, 'x',  'cov3d_coarse.mat', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covmat_coarse = pyemu.Cov.from_ascii(os.path.join(template_ws, \"cov3d_coarse.mat\"))\n",
    "\n",
    "# This covaraince matrix can now be used as the base for all pilot point parameters. \n",
    "# Note that in this case the variance is 1, so it is easy to scale to a parameters prior varaince\n",
    "# Depending on how you setup the scrpt, variance can be assigned at various stages (i.e. when running PPCOV_SVA, or by manipulating the matrix later)\n",
    "# Note that parameter names (headers and row names) come from the parameter name sin the ppoint file. These can have a prefix added by PPCOV_SVA, or changed in the dataframe. The latter is more versatile.\n",
    "covmat_coarse.to_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_parameters(\n",
    "    filenames=f'{ml_name}.npf_k_layer1.txt',\n",
    "    par_type='pilotpoints',\n",
    "    par_name_base=\"hk_layer_1\",\n",
    "    pargp=\"hk_layer_1\",\n",
    "    #zone_array=ib,\n",
    "    upper_bound=864.0,\n",
    "    lower_bound=0.864,\n",
    "    ult_ubound=1200.0,\n",
    "    ult_lbound=0.00001,\n",
    "    transform='log',\n",
    "    #geostruct=covmat_coarse, AttributeError: 'Cov' object has no attribute 'sill'\n",
    "    pp_space=pp_coarse3d.loc[pp_coarse3d['layer'] == 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Head Boundaries (GHB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbpp = pd.read_csv(os.path.join(tmp_model_ws, 'ghbpp.dat'), sep='\\t')\n",
    "ghbpp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbpp['zone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbpp = ghbpp[['ppid', 'x', 'y', 'zone', 'cond', 'bnames']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbpp.rename(columns={'cond': 'parval1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbppred = ghbpp.loc[ghbpp['bnames'] == 'red'][['ppid', 'x', 'y', 'zone', 'parval1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbppred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_parameters(\n",
    "    filenames=f'{ml_name}.ghb_stress_period_data_1.txt', \n",
    "    par_name_base='ghbr_rhk',\n",
    "    pargp='ghb_hk',\n",
    "    index_cols=[0,1], \n",
    "    use_cols=[3],\n",
    "    upper_bound=10.,\n",
    "    lower_bound=0.1,\n",
    "    ult_ubound=100.,\n",
    "    ult_lbound=0.01,\n",
    "    par_type='pilotpoints',\n",
    "    pp_space=ghbppred\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Flow Routing (SFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add grid-scale parameters for SFR reach conductance.  \n",
    "# Use layer, row, col and reach number in the \n",
    "# parameter names\n",
    "pf.add_parameters(\n",
    "    filenames=f'{ml_name}.sfr_packagedata.txt', \n",
    "    par_name_base='sfr_rhk',\n",
    "    pargp='sfr_rhk',\n",
    "    index_cols=[0,1,2], \n",
    "    use_cols=[9],\n",
    "    upper_bound=10.,\n",
    "    lower_bound=0.1,\n",
    "    ult_ubound=100.,\n",
    "    ult_lbound=0.01,\n",
    "    par_type='grid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d.loc[pp_coarse3d['layer'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add observations\n",
    "\n",
    "### 2.1 Heads\n",
    "\n",
    "#### Computed head\n",
    "\n",
    "- Is there a point of having \"hds_usecol:\" in the obsnme and obgnme or can this be removed safely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws, 'head.obs.csv'),index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_start = pd.to_datetime('2016-01-01') + pd.to_timedelta(df.index.values,unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_file = os.path.join(tmp_model_ws, 'head.obs.csv')\n",
    "\n",
    "hds_df = pf.add_observations(\n",
    "    hds_file,\n",
    "    insfile='head.csv.ins',\n",
    "    index_cols='time',\n",
    "    use_cols=list(df.columns.values),\n",
    "    prefix='hds'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set weights to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df.loc[:,'weight'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measured head\n",
    "The plan is to change the `obsval` of `hds_df` with real, measured data. Let's load the field measured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas = pyemu.utils.smp_to_dataframe(os.path.join('..', 'data', 'olproc_input', 'obs-head-all.ssf'))\n",
    "hds_meas['name'] = hds_meas['name'].str.lower()\n",
    "hds_meas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot measured heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for site in hds_meas['name'].unique():\n",
    "    \n",
    "    site_obs_data = hds_meas.loc[hds_meas['name'] == site]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    \n",
    "    ax.plot(site_obs_data.datetime,site_obs_data.value)\n",
    "    ax.set_title(site)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets will now be sampled for use in history-matching purposes. Sampling can be done in different ways. Let's explore two options based on a notebook developed by Jeremy White (https://github.com/jtwhite79/decision_support_analysis_notebooks/blob/main/notebooks/process_obs_and_set_weights.ipynb) and visualize the options together with the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: sampling the nearest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_nearest, hds_meas_lowpass = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in hds_meas['name'].unique():\n",
    "    site_obs_data = hds_meas.loc[hds_meas['name'] == site].copy()\n",
    "    site_obs_data.index = site_obs_data.datetime\n",
    "    site_obs_data = site_obs_data.reindex(sp_start,method='nearest')\n",
    "    site_obs_data['datetime'] = site_obs_data.index\n",
    "    site_obs_data.sort_values(by='datetime', inplace=True)\n",
    "    hds_meas_nearest.append(site_obs_data)\n",
    "hds_meas_nearest = pd.concat(hds_meas_nearest,axis=0,ignore_index=True)\n",
    "hds_meas_nearest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe now contain alot of duplicate values (as is apparent below). Let's clean this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_nearest = hds_meas_nearest.drop_duplicates('value').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: using a lowpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in hds_meas['name'].unique():\n",
    "    site_obs_data = hds_meas.loc[hds_meas['name'] == site].copy()\n",
    "    site_obs_data.index = site_obs_data.datetime\n",
    "    \n",
    "    sm = site_obs_data.value.rolling(window=60,center=True,min_periods=1).mean()\n",
    "    sm_site_obs_data = sm.reindex(sp_start,method='nearest')\n",
    "\n",
    "    df = pd.DataFrame(sm_site_obs_data)\n",
    "    df['datetime'], df['name'] = df.index, site\n",
    "    df = df[['datetime', 'name', 'value']]\n",
    "    df.sort_values(by=\"datetime\",inplace=True)\n",
    "    hds_meas_lowpass.append(df)\n",
    "hds_meas_lowpass = pd.concat(hds_meas_lowpass,axis=0,ignore_index=True).drop_duplicates('value').reset_index(drop=True)\n",
    "hds_meas_lowpass.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that sampled data is only collected from time-periods of when actual measurements were collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sites = hds_meas['name'].unique().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timedeltas(df, model_start_time):\n",
    "    df['timedelta'] = [(dt - pd.to_datetime(model_start_time)).total_seconds() / 86400 for dt in df.datetime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [hds_meas, hds_meas_nearest, hds_meas_lowpass]:\n",
    "    add_timedeltas(df, '2016-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_timedeltas(measured_df, timeinterp_df):\n",
    "    '''\n",
    "    This function compares timedeltas of the time-interpolated values with that of the field measurements.\n",
    "    \n",
    "    If a time-interpolated value is within +- 2 days of a field measurement,\n",
    "    this value is accepted, else the row is dropped.\n",
    "    '''\n",
    "    timedelta_match = []\n",
    "    \n",
    "    for i, r in timeinterp_df.iterrows():\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for j in measured_df.timedelta:\n",
    "            result = j + 2 >= r.timedelta >= j - 2 # determine whether there is a match between datasets\n",
    "            results.append(result)\n",
    "\n",
    "        if any(results) == True:\n",
    "            timedelta_match.append(True)\n",
    "        else:\n",
    "            timedelta_match.append(False)\n",
    "            \n",
    "    return timedelta_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [hds_meas_nearest, hds_meas_lowpass]:\n",
    "    df['td_match'] = False\n",
    "    \n",
    "    # Loop over each site\n",
    "    for site in hds_meas.name.unique():\n",
    "        meas_df = hds_meas.loc[hds_meas['name'] == site]\n",
    "        df_sub = df.loc[df['name'] == site]\n",
    "        \n",
    "        df.loc[df['name'] == site, 'td_match'] = match_timedeltas(meas_df, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_nearest = hds_meas_nearest.loc[hds_meas_nearest['td_match'] == True].reset_index(drop=True)\n",
    "hds_meas_lowpass = hds_meas_lowpass.loc[hds_meas_lowpass['td_match'] == True].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the two sampling options. `Nearest data` samples are shown in red. `Lowpass data` samples are shown in green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in obs_sites:\n",
    "    \n",
    "    site_obs_data = hds_meas.loc[hds_meas['name'] == site]\n",
    "    site_obs_data_nearest = hds_meas_nearest.loc[hds_meas_nearest['name'] == site]\n",
    "    site_obs_data_lowpass = hds_meas_lowpass.loc[hds_meas_lowpass['name'] == site]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    \n",
    "    ax.plot(site_obs_data.datetime,site_obs_data.value)\n",
    "    ax.plot(site_obs_data_nearest.datetime,site_obs_data_nearest.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.scatter(site_obs_data_nearest.datetime,site_obs_data_nearest.value, color='r')\n",
    "\n",
    "    ax.plot(site_obs_data_lowpass.datetime, site_obs_data_lowpass.value,'g-',lw=1,marker='.',ms=10)\n",
    "    ax.scatter(site_obs_data_lowpass.datetime, site_obs_data_lowpass.value, color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowpass sampling method seem to capture the trend of the data better than using the nearest option. Because of this, the lowpass interpolated measurements will be used in the history-matching process.\n",
    "\n",
    "#### Replace model `obsval` with `lowpass` interpolated measurements\n",
    "A reminder of how the PEST observations are setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the mission is to assign a weight > 0 for the observations where we have time-interpolated (using a lowpass filter) measurements. Recall that the time-interpolated measurements are located in the `hds_meas_lowpass` DataFrame. The values in this dataframe must now be matched with the corresponding values of `hds_df`.\n",
    "\n",
    "As we can see, we have a `datetime` column. This information must be converted into model time so that values can be matched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_lowpass.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matching_obs_columns(df, col_prefix):\n",
    "    \n",
    "    # Build partial obsnme column (for matching against PEST observations dataframe)\n",
    "    df['matchcol'] = [f'{col_prefix}_usecol:{i}_time:{j}' for i, j in zip(df['name'], df['timedelta'])]\n",
    "    df['matchcol'] = [i[:i.index('.') + 2] for i in df['matchcol']]\n",
    "    \n",
    "    df['obsgnme'] = [f'{col_prefix}_usecol:{i}' for i in df['name']]\n",
    "    \n",
    "    df['weight'] = 1.0\n",
    "    \n",
    "    df['obsval'] = df['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_matching_obs_columns(hds_meas_lowpass, 'hds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`obsnme` in the `hds_meas_lowpass` dataframe will now be used to find matching `obsnme` rows in `hds_df` and then replace `obsval` and `weight` with time-interpolated field measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_lowpass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a temporary `matchcol` column so that `obsval` can be changed to time-interpolated measured counterparts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df['matchcol'] = [i[:i.index('.') + 2] for i in hds_df['obsnme']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the time-interpolated measurements to the `obsval` and give these measurements a positive weight for the history-matching process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in hds_meas_lowpass.iterrows():\n",
    "\n",
    "    hds_df.loc[hds_df.matchcol == r.matchcol, 'weight'] = 1.0\n",
    "    hds_df.loc[hds_df.matchcol == r.matchcol, 'obsval'] = r.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df.loc[hds_df['weight'] == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the temporary `matchcol` column from the history-matching dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df.drop('matchcol', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up history-matching for heads completed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Head differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the csv-file from measured head that will be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws, 'head.obs.csv'),index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in df.iterrows():\n",
    "    print(r.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_lowpass_td = hds_meas_lowpass.copy() # temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_lowpass_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read computed stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws, 'sfr_stage.obs.csv'),index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_file = os.path.join(template_ws, 'sfr_stage.obs.csv')\n",
    "\n",
    "stg_df = pf.add_observations(\n",
    "    stg_file,\n",
    "    insfile='stage.csv.ins',\n",
    "    index_cols='time',\n",
    "    use_cols=list(df.columns.values),\n",
    "    prefix='stg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_df.loc[:,'weight'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_meas = pyemu.utils.smp_to_dataframe(os.path.join('..', 'data', 'olproc_input', 'obs-stage1.ssf'))\n",
    "stg_meas['name'] = stg_meas['name'].str.lower()\n",
    "stg_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in stg_meas['name'].unique():\n",
    "    \n",
    "    site_obs_data = stg_meas.loc[stg_meas['name'] == site]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    \n",
    "    ax.plot(site_obs_data.datetime,site_obs_data.value)\n",
    "    ax.set_title(site)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_meas_nearest, stg_meas_lowpass = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in stg_meas['name'].unique():\n",
    "    site_obs_data = stg_meas.loc[stg_meas['name'] == site].copy()\n",
    "    site_obs_data.index = site_obs_data.datetime\n",
    "    site_obs_data = site_obs_data.reindex(sp_start,method='nearest')\n",
    "    site_obs_data['datetime'] = site_obs_data.index\n",
    "    site_obs_data.sort_values(by='datetime', inplace=True)\n",
    "    stg_meas_nearest.append(site_obs_data)\n",
    "stg_meas_nearest = pd.concat(stg_meas_nearest,axis=0,ignore_index=True).drop_duplicates('value').reset_index(drop=True)\n",
    "stg_meas_nearest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in stg_meas['name'].unique():\n",
    "    site_obs_data = stg_meas.loc[stg_meas['name'] == site].copy()\n",
    "    site_obs_data.index = site_obs_data.datetime\n",
    "    \n",
    "    sm = site_obs_data.value.rolling(window=30,center=True,min_periods=1).mean() # test window of 30 inst of 60\n",
    "    sm_site_obs_data = sm.reindex(sp_start,method='nearest')\n",
    "\n",
    "    df = pd.DataFrame(sm_site_obs_data)\n",
    "    df['datetime'], df['name'] = df.index, site\n",
    "    df = df[['datetime', 'name', 'value']]\n",
    "    df.sort_values(by=\"datetime\",inplace=True)\n",
    "    stg_meas_lowpass.append(df)\n",
    "stg_meas_lowpass = pd.concat(stg_meas_lowpass,axis=0,ignore_index=True).drop_duplicates('value').reset_index(drop=True)\n",
    "stg_meas_lowpass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sites = stg_meas['name'].unique().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [stg_meas, stg_meas_nearest, stg_meas_lowpass]:\n",
    "    add_timedeltas(df, '2016-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [stg_meas_nearest, stg_meas_lowpass]:\n",
    "    df['td_match'] = False\n",
    "    \n",
    "    # Loop over each site\n",
    "    for site in stg_meas.name.unique():\n",
    "        meas_df = stg_meas.loc[stg_meas['name'] == site]\n",
    "        df_sub = df.loc[df['name'] == site]\n",
    "        \n",
    "        df.loc[df['name'] == site, 'td_match'] = match_timedeltas(meas_df, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_meas_nearest = stg_meas_nearest.loc[stg_meas_nearest['td_match'] == True].reset_index(drop=True)\n",
    "stg_meas_lowpass = stg_meas_lowpass.loc[stg_meas_lowpass['td_match'] == True].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in obs_sites:\n",
    "    \n",
    "    site_obs_data = stg_meas.loc[stg_meas['name'] == site]\n",
    "    site_obs_data_nearest = stg_meas_nearest.loc[stg_meas_nearest['name'] == site]\n",
    "    site_obs_data_lowpass = stg_meas_lowpass.loc[stg_meas_lowpass['name'] == site]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    \n",
    "    ax.plot(site_obs_data.datetime,site_obs_data.value)\n",
    "    ax.plot(site_obs_data_nearest.datetime,site_obs_data_nearest.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.scatter(site_obs_data_nearest.datetime,site_obs_data_nearest.value, color='r')\n",
    "\n",
    "    ax.plot(site_obs_data_lowpass.datetime, site_obs_data_lowpass.value,'g-',lw=1,marker='.',ms=10)\n",
    "    ax.scatter(site_obs_data_lowpass.datetime, site_obs_data_lowpass.value, color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_meas_lowpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_matching_obs_columns(stg_meas_lowpass, 'stg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_meas_lowpass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_df['matchcol'] = [i[:i.index('.') + 2] for i in stg_df['obsnme']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in stg_meas_lowpass.iterrows():\n",
    "\n",
    "    stg_df.loc[stg_df.matchcol == r.matchcol, 'weight'] = 1.0\n",
    "    stg_df.loc[stg_df.matchcol == r.matchcol, 'obsval'] = r.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_df.loc[stg_df['weight'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_df.drop('matchcol', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up history-matching for stage completed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Stage differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Inflow (Stream gage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws, 'sfr_gage.obs.csv'),index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_file = os.path.join(tmp_model_ws, 'sfr_gage.obs.csv')\n",
    "\n",
    "inf_df = pf.add_observations(\n",
    "    inf_file,\n",
    "    insfile='sfr_gage.csv.ins',\n",
    "    index_cols='time',\n",
    "    use_cols=list(df.columns.values),\n",
    "    prefix='inf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.loc[:,'weight'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_meas = pyemu.utils.smp_to_dataframe(os.path.join('..', 'data', 'olproc_input', 'obs-gage1.ssf'))\n",
    "inf_meas['name'] = inf_meas['name'].str.lower()\n",
    "inf_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in inf_meas['name'].unique():\n",
    "    \n",
    "    site_obs_data = inf_meas.loc[inf_meas['name'] == site]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    \n",
    "    ax.plot(site_obs_data.datetime,site_obs_data.value)\n",
    "    ax.set_title(site)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_meas_nearest, inf_meas_lowpass = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in inf_meas['name'].unique():\n",
    "    site_obs_data = inf_meas.loc[inf_meas['name'] == site].copy()\n",
    "    site_obs_data.index = site_obs_data.datetime\n",
    "    site_obs_data = site_obs_data.reindex(sp_start,method='nearest')\n",
    "    site_obs_data['datetime'] = site_obs_data.index\n",
    "    site_obs_data.sort_values(by='datetime', inplace=True)\n",
    "    inf_meas_nearest.append(site_obs_data)\n",
    "inf_meas_nearest = pd.concat(inf_meas_nearest,axis=0,ignore_index=True).drop_duplicates('value').reset_index(drop=True)\n",
    "inf_meas_nearest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in inf_meas['name'].unique():\n",
    "    site_obs_data = inf_meas.loc[inf_meas['name'] == site].copy()\n",
    "    site_obs_data.index = site_obs_data.datetime\n",
    "    \n",
    "    sm = site_obs_data.value.rolling(window=30,center=True,min_periods=1).mean() #30 window\n",
    "    sm_site_obs_data = sm.reindex(sp_start,method='nearest')\n",
    "\n",
    "    df = pd.DataFrame(sm_site_obs_data)\n",
    "    df['datetime'], df['name'] = df.index, site\n",
    "    df = df[['datetime', 'name', 'value']]\n",
    "    df.sort_values(by='datetime', inplace=True)\n",
    "    inf_meas_lowpass.append(df)\n",
    "inf_meas_lowpass = pd.concat(inf_meas_lowpass,axis=0,ignore_index=True).drop_duplicates('value').reset_index(drop=True)\n",
    "inf_meas_lowpass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sites = inf_meas['name'].unique().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [inf_meas, inf_meas_nearest, inf_meas_lowpass]:\n",
    "    add_timedeltas(df, '2016-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [inf_meas_nearest, inf_meas_lowpass]:\n",
    "    df['td_match'] = False\n",
    "    \n",
    "    # Loop over each site\n",
    "    for site in inf_meas.name.unique():\n",
    "        meas_df = inf_meas.loc[inf_meas['name'] == site]\n",
    "        df_sub = df.loc[df['name'] == site]\n",
    "        \n",
    "        df.loc[df['name'] == site, 'td_match'] = match_timedeltas(meas_df, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_meas_nearest = inf_meas_nearest.loc[inf_meas_nearest['td_match'] == True].reset_index(drop=True)\n",
    "inf_meas_lowpass = inf_meas_lowpass.loc[inf_meas_lowpass['td_match'] == True].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in obs_sites:\n",
    "    \n",
    "    site_obs_data = inf_meas.loc[inf_meas['name'] == site]\n",
    "    site_obs_data_nearest = inf_meas_nearest.loc[inf_meas_nearest['name'] == site]\n",
    "    site_obs_data_lowpass = inf_meas_lowpass.loc[inf_meas_lowpass['name'] == site]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    \n",
    "    ax.plot(site_obs_data.datetime,site_obs_data.value)\n",
    "    ax.plot(site_obs_data_nearest.datetime,site_obs_data_nearest.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.scatter(site_obs_data_nearest.datetime,site_obs_data_nearest.value, color='r')\n",
    "\n",
    "    ax.plot(site_obs_data_lowpass.datetime, site_obs_data_lowpass.value,'g-',lw=1,marker='.',ms=10)\n",
    "    ax.scatter(site_obs_data_lowpass.datetime, site_obs_data_lowpass.value, color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_meas_lowpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_matching_obs_columns(inf_meas_lowpass, 'inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_meas_lowpass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df['matchcol'] = [i[:i.index('.') + 2] for i in inf_df['obsnme']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in inf_meas_lowpass.iterrows():\n",
    "\n",
    "    inf_df.loc[inf_df.matchcol == r.matchcol, 'weight'] = 1.0\n",
    "    inf_df.loc[inf_df.matchcol == r.matchcol, 'obsval'] = r.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.loc[inf_df['weight'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.drop('matchcol', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(inf_df['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Inflow differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Head-Stage differences\n",
    "\n",
    "This is a customized measurement type\n",
    "\n",
    "Start by creating the synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_comp = pd.read_csv(os.path.join(template_ws, 'head.obs.csv'),index_col=0)\n",
    "hds_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_comp = pd.read_csv(os.path.join(template_ws, 'sfr_stage.obs.csv'),index_col=0)\n",
    "stg_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'NI15-O1', 'NI15-O1-STG',\n",
    "    'NI15-O44', 'NI15-O44-STG',\n",
    "    'NI15-O46', 'NI15-O46-STG',\n",
    "    'NI15-O47', 'NI15-O47-STG',\n",
    "    'NI15-O48', 'NI15-O48-STG'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = pd.concat([hds_comp, stg_comp], axis=1)[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(columns[::2], columns[1::2]):\n",
    "    dh[f'dh{i}'] = dh[i] - dh[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dh[[f'dh{i}' for i in columns[::2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.to_csv(os.path.join(template_ws, 'dh_orbacken.obs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dho_file = os.path.join(template_ws, 'dh_orbacken.obs.csv')\n",
    "\n",
    "dho_df = pf.add_observations( #delta h rbcken (head stage difference at 5 points)\n",
    "    dho_file,\n",
    "    insfile='dh_orbacken.csv.ins',\n",
    "    index_cols='time',\n",
    "    use_cols=list(dh.columns.values),\n",
    "    prefix='dho'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dho_df['weight'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dho_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root = os.path.join('..', 'temp_ml_param')\n",
    "\n",
    "runmodel_ws = os.path.join(model_root, 'runmodel')\n",
    "model_ws = os.path.join(runmodel_ws, 'model')\n",
    "preproc_ws = os.path.join(runmodel_ws, 'preproc')\n",
    "lumprem_ws = os.path.join(runmodel_ws, 'lumprem')\n",
    "\n",
    "pest_ws = os.path.join(model_root, 'pest')\n",
    "template_ws = os.path.join(pest_ws, 'template')\n",
    "instruction_ws = os.path.join(pest_ws, 'instruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_lvl1 = [runmodel_ws, pest_ws]\n",
    "dir_lvl2 = [model_ws, preproc_ws, lumprem_ws, template_ws, instruction_ws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_root):\n",
    "    shutil.rmtree(model_root)\n",
    "\n",
    "os.mkdir(model_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pth in dir_lvl1:\n",
    "    os.mkdir(pth)\n",
    "for pth in dir_lvl2:\n",
    "    os.mkdir(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copytree(org_model_ws,model_ws, dirs_exist_ok=True)\n",
    "os.listdir(model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move all the LUMPREM template files to `template_ws` and SEGLIST files to `preproc_ws` directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(old_pth, new_pth, file):\n",
    "    from_ = os.path.join(old_pth, file)\n",
    "    to_ = os.path.join(new_pth, file)\n",
    "    shutil.move(from_, to_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tpls = [i for i in os.listdir(model_ws) if '.tpl' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in lr_tpls:\n",
    "    move_files(model_ws, template_ws, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(model_ws, preproc_ws, 'sfr_segfile.dat')\n",
    "move_files(model_ws, preproc_ws, 'ghbpp.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move all the LUMPREM input files to `lumprem_ws`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prep_pth = os.path.join('..', 'temp_lr_prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(lr_prep_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(lr_prep_pth):\n",
    "    shutil.copyfile(os.path.join(lr_prep_pth, file), os.path.join(lumprem_ws, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(lumprem_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_name = 'hagfors_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(ml_name, 'mf6', os.path.join(bins_pth, 'mf6'), model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all data external so that PEST can adjust parameter values during history matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.set_all_data_external(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.write_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwf = sim.get_model(ml_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "plt.ticklabel_format(axis='both', style='plain', useOffset=False) #Show coordinates\n",
    "ax.set_title('Model grid', fontsize=18)\n",
    "\n",
    "mapview = flopy.plot.PlotMapView(gwf, layer=0)\n",
    "linecollection = mapview.plot_grid(lw=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pilot points\n",
    "3D-pilot points will be generated for the following areal properties:\n",
    "- SY & SS (perhaps SS could be skipped?)\n",
    "- Porosity and possibly diffusion/dispersion and any other parameter needed to represent transport\n",
    "- Kh and Kz\n",
    "\n",
    "In addition, pilot points will be generated along the following linear features:\n",
    "- Streambed hydraulic conductivity along Creek rbcken (SFR package)\n",
    "- GHB conductance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D pilot points\n",
    "Because we want to do a data-worth analysis, we should consider the total amount of pilot points to be employed to find a suitable compromise between adjustable parameters and model run-time.\n",
    "\n",
    "For this reason, we will create different sets of pilot points for each parameter type (in order to not use an exessive amount of pps).\n",
    "\n",
    "Sy, SS and possibly porosity (along with other parameters that govern transport) could be parameterized using a coarser pp-spacing, so let's start with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before we start, let's import the observation locations, so that we can see how the pilot points will be located in relation to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_obs = pd.read_excel(os.path.join(observations_pth, 'obs_flow_and_stage.xlsx'))\n",
    "head_obs = pd.read_excel(os.path.join(observations_pth, 'obs_head_per_layer.xlsx')).drop_duplicates(subset=['POINT_X', 'POINT_Y'])\n",
    "head_obs['TYPE'] = 'HEAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_points = pd.concat([head_obs, sfr_obs])\n",
    "display(obs_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse 3D pilot points\n",
    "Let's create the coarse pilot point distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_boundary = shapefile.Reader(os.path.join(shapefile_pth, 'model_boundary.shp')) # Model boundary shapefile\n",
    "mlb_shape = np.array(np.rint(ml_boundary.shapeRecords()[0].shape.points)) # Model boundary array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a shapely polygon of the model boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_shapely = Polygon(mlb_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prepared polygon of the model boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlb_shapely_prep = prep(mlb_shapely.buffer(50)) buffer cause an issue because the grid can't be sampled outside the model boundary\n",
    "mlb_shapely_prep = prep(mlb_shapely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a rectangular mesh of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, ymin, ymax = 426900, 427700, 6654650, 6655350 # These are the same coordinates used to construct the base-grid\n",
    "resolution = 30 # Equal space (in meters) between pilot points\n",
    "basepoints = []\n",
    "for lat in np.arange(xmin, xmax, resolution):\n",
    "    for lon in np.arange(ymin, ymax, resolution):\n",
    "        basepoints.append(Point((round(lat,4), round(lon,4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the shapely `contains` (point-in-polygon method) to select points inside the model boundary (increase the number of pps once workflow is confirmed working):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepip = [] # Basepoints in polygon\n",
    "for point in basepoints:\n",
    "    if mlb_shapely_prep.contains(point):\n",
    "        basepip.append(point)\n",
    "print(f'Number of points per layer: {len(basepip)}') # We need to extend it into three dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the position of the pilot points on top of the model grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "plt.ticklabel_format(axis='both', style='plain', useOffset=False) #Show coordinates\n",
    "ax.set_title('Coarse pilot point locations', fontsize=18)\n",
    "\n",
    "mapview = flopy.plot.PlotMapView(gwf, layer=0)\n",
    "linecollection = mapview.plot_grid(lw=0.25)\n",
    "\n",
    "x = np.array([i.coords[0] for i in basepip])[:,0]\n",
    "y = np.array([i.coords[0] for i in basepip])[:,1]\n",
    "plt.scatter(x, y, s=5, c='black', alpha=0.5, label='PILOT POINT')\n",
    "\n",
    "for category in obs_points['TYPE'].unique():\n",
    "    x = obs_points.loc[obs_points['TYPE'] == category]['POINT_X']\n",
    "    y = obs_points.loc[obs_points['TYPE'] == category]['POINT_Y']\n",
    "    ax.scatter(x, y, s=20, alpha=0.6, label=category)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These points now need to be assigned a z-value, since we are going to use 3D-pilot points. To do this, we will need to intersect the model grid and retrieve the z-values of each layer, so that a copy of these pps can be positioned in each of the three layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = GridIntersect(gwf.modelgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect = {\n",
    "    'cellids': [],\n",
    "    'vertices': [],\n",
    "    'ixshapes': [],\n",
    "}\n",
    "for point in basepip:\n",
    "    pp_intersect['cellids'].append(ix.intersect(point).cellids[0])\n",
    "    pp_intersect['vertices'].append(ix.intersect(point).vertices[0])\n",
    "    pp_intersect['ixshapes'].append(ix.intersect(point).ixshapes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect = pd.DataFrame(pp_intersect)\n",
    "display(pp_intersect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the order is respected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.coords[0] for i in basepip][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the cellids to sample pilot point elevations (**this takes about 2 minutes on my laptop for 295 cells** and could/should probably be speed up somehow - considering it has to be done for top, botm1, botm2 and botm3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_elevation(elevation_array, index_array):\n",
    "    '''\n",
    "    returns list of elevations\n",
    "    '''\n",
    "    \n",
    "    elevations = list(elevation_array)\n",
    "    indices = list(index_array)\n",
    "    \n",
    "    return [elevation_array[i] for i in index_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['top'] = get_grid_elevation(elevation_array = gwf.modelgrid.top, index_array=pp_intersect['cellids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['botm_1'] = get_grid_elevation(gwf.modelgrid.botm[0], pp_intersect['cellids'])\n",
    "pp_intersect['botm_2'] = get_grid_elevation(gwf.modelgrid.botm[1], pp_intersect['cellids'])\n",
    "pp_intersect['botm_3'] = get_grid_elevation(gwf.modelgrid.botm[2], pp_intersect['cellids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pps in-between the layer boundaries (i.e. vertically centered in the cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['pps_l1'] = (pp_intersect['top'] + pp_intersect['botm_1']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['pps_l2'] = (pp_intersect['botm_1'] + pp_intersect['botm_2']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['pps_l3'] = (pp_intersect['botm_2'] + pp_intersect['botm_3']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pp_intersect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add x, y and clean up df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect['x'] = np.array([i.coords[0] for i in basepip])[:,0]\n",
    "pp_intersect['y'] = np.array([i.coords[0] for i in basepip])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect = pp_intersect[['x', 'y', 'pps_l1', 'pps_l2', 'pps_l3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "\n",
    "fig = pyplot.figure(figsize=(6,6))\n",
    "ax = Axes3D(fig)\n",
    "ax.set_box_aspect([2,2,1])\n",
    "\n",
    "x, y = pp_intersect.x.values, pp_intersect.y.values\n",
    "\n",
    "ax.scatter(x, y, pp_intersect.pps_l1.values)\n",
    "ax.scatter(x, y, pp_intersect.pps_l2.values)\n",
    "ax.scatter(x, y, pp_intersect.pps_l3.values)\n",
    "plt.title('pps in 3d-space')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataframe into a 3d pilot point file format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d = pd.melt(pp_intersect, id_vars=['x', 'y'], value_vars=['pps_l1', 'pps_l2', 'pps_l3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "\n",
    "fig = pyplot.figure(figsize=(6,6))\n",
    "ax = Axes3D(fig)\n",
    "ax.set_box_aspect([2,2,1])\n",
    "\n",
    "x, y = pp_coarse3d.x.values, pp_coarse3d.y.values\n",
    "\n",
    "ax.scatter(x, y, pp_coarse3d.value.values)\n",
    "plt.title('pps in 3d-space')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d['name'] = [f'ppc{i:04d}' for i in pp_coarse3d.index.values] # ppc = pilot point coarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d[['zone','val']] = 1, 1\n",
    "pp_coarse3d['z'] = pp_coarse3d['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d['layer'] = [int(i[-1]) for i in pp_coarse3d['variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d = pp_coarse3d[['name', 'x', 'y', 'z', 'zone', 'val', 'layer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pp_coarse3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumpyrem import run\n",
    "#MKPPSTAT\n",
    "# mkppstat requires no headers in ppoint file ...sigh...\n",
    "pp_coarse3d.to_csv(os.path.join(preproc_ws, 'mkppoints3d_coarse.dat'),\n",
    "                       header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for MKPPSTAT\n",
    "# A (a)-factor of 1.5 is often reasonable. from tutorial \n",
    "npoints_h, npoints_v = 10, 10 # np-horizontal, np-vertical\n",
    "a_h, a_v = 1.2, 1.2 # a-horizontal, a-vertical\n",
    "# run MKPPSTAT\n",
    "run.run_process(\n",
    "    'mkppstat3d',\n",
    "    path=preproc_ws,\n",
    "    commands=['mkppoints3d_coarse.dat', npoints_h, a_h, npoints_v, a_v, 'ppstat3d_coarse.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PPCOV3D_SVA - pilot point covariance 3d - spatially varying anisotropy\n",
    "run.run_process(\n",
    "    'ppcov3d_sva',\n",
    "    path=preproc_ws,\n",
    "    commands=['ppstat3d_coarse.dat', 'y', 1, 'x',  'cov3d_coarse.mat', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cov mat file with Pyemu for further processing\n",
    "import pyemu\n",
    "covmat_coarse = pyemu.Cov.from_ascii(os.path.join(preproc_ws, \"cov3d_coarse.mat\"))\n",
    "\n",
    "# This covaraince matrix can now be used as the base for all pilot point parameters. \n",
    "# Note that in this case the variance is 1, so it is easy to scale to a parameters prior varaince\n",
    "# Depending on how you setup the scrpt, variance can be assigned at various stages (i.e. when running PPCOV_SVA, or by manipulating the matrix later)\n",
    "# Note that parameter names (headers and row names) come from the parameter name sin the ppoint file. These can have a prefix added by PPCOV_SVA, or changed in the dataframe. The latter is more versatile.\n",
    "covmat_coarse.to_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this matrix only used during regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8), dpi=100)\n",
    "plt.imshow(covmat_coarse.as_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d[['kh', 'kv', 'sy', 'ss']] = 86.4, 8.64, 0.2, 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d.to_csv(os.path.join(preproc_ws, 'pp3d_coarse.dat'), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameterization for 3D elements (K, storage, porosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write generic template\n",
    "with open(os.path.join(preproc_ws, 'gen_mf_array.tpl'), 'a') as f:\n",
    "    f.write('$#p prop_mf.write_in_sequence(format=\"(1x,1pg18.11)\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_script(filename, lines):\n",
    "\n",
    "    with open(filename, 'a') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate kriging factors (porosity not included yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(os.path.join(preproc_ws, 'plproc.dat'), [\n",
    "f'''\n",
    "### Read model grid ###\n",
    "cl_mf = read_mf6_grid_specs(file=../model/{ml_name}.disv.grb,                  &\n",
    "                            dimensions=2,                             &\n",
    "                            slist_layer_idomain=idomain1;  layer=1,   &\n",
    "                            slist_layer_idomain=idomain2;  layer=2,   &\n",
    "                            slist_layer_idomain=idomain3;  layer=3)\n",
    "\n",
    "\n",
    "\n",
    "### Read 3D pilot-points file ###\n",
    "cl_pp = read_list_file(file=pp3d_coarse.dat,               &\n",
    "                       id_type=character,                  &\n",
    "                       dimensions=2,                       &\n",
    "                       skiplines=1,                        &\n",
    "                       slist=zone; col=5,                  &\n",
    "                       slist=lyr; col=7,                   &\n",
    "                       plist=kh_pp; col=8,                 &\n",
    "                       plist=kv_pp; col=9,                 &\n",
    "                       plist=sy_pp; col=10,                &\n",
    "                       plist=ss_pp; col=11)\n",
    "\n",
    "\n",
    "### Calculate kriging factors for each layer ###\n",
    "calc_kriging_factors_auto_2d(target_clist=cl_mf, source_clist=cl_pp;select=(lyr==1), file=factors_pp_lyr1.dat)\n",
    "\n",
    "calc_kriging_factors_auto_2d(target_clist=cl_mf, source_clist=cl_pp;select=(lyr==2), file=factors_pp_lyr2.dat)\n",
    "\n",
    "calc_kriging_factors_auto_2d(target_clist=cl_mf, source_clist=cl_pp;select=(lyr==3), file=factors_pp_lyr3.dat)\n",
    "\n",
    "### Write to template file ###\n",
    "prop_mf=new_plist(reference_clist=cl_mf,value=1.0)\n",
    "\n",
    "###   Horizontal K   ###\n",
    "### Write kh layer 1 ###\n",
    "prop_mf=86.4\n",
    "prop_mf=kh_pp.krige_using_file(file='factors_pp_lyr1.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.npf_k_layer1.txt)\n",
    "\n",
    "### Write kh layer 2 ###\n",
    "prop_mf=86.4\n",
    "prop_mf=kh_pp.krige_using_file(file='factors_pp_lyr2.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.npf_k_layer2.txt)\n",
    "\n",
    "### Write kh layer 3 ###\n",
    "prop_mf=86.4\n",
    "prop_mf=kh_pp.krige_using_file(file='factors_pp_lyr3.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.npf_k_layer3.txt)\n",
    "\n",
    "\n",
    "###    Vertical K    ###\n",
    "### Write kv layer 1 ###\n",
    "prop_mf=8.64\n",
    "prop_mf=kv_pp.krige_using_file(file='factors_pp_lyr1.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.npf_k33_layer1.txt)\n",
    "\n",
    "### Write kv layer 2 ###\n",
    "prop_mf=8.64\n",
    "prop_mf=kv_pp.krige_using_file(file='factors_pp_lyr2.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.npf_k33_layer2.txt)\n",
    "\n",
    "### Write kv layer 3 ###\n",
    "prop_mf=8.64\n",
    "prop_mf=kv_pp.krige_using_file(file='factors_pp_lyr3.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.npf_k33_layer3.txt)\n",
    "\n",
    "\n",
    "\n",
    "###      STORAGE     ###\n",
    "###        sy        ###\n",
    "### Write sy layer 1 ###\n",
    "prop_mf=0.2\n",
    "prop_mf=sy_pp.krige_using_file(file='factors_pp_lyr1.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.sto_sy_layer1.txt)\n",
    "\n",
    "\n",
    "###        ss        ###\n",
    "### Write ss layer 2 ###\n",
    "prop_mf=0.000001\n",
    "prop_mf=ss_pp.krige_using_file(file='factors_pp_lyr2.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.sto_ss_layer2.txt)\n",
    "\n",
    "### Write ss layer 3 ###\n",
    "prop_mf=0.000001\n",
    "prop_mf=ss_pp.krige_using_file(file='factors_pp_lyr3.dat',transform='log')\n",
    "write_model_input_file(template_file=gen_mf_array.tpl, &\n",
    "    model_input_file=../model/{ml_name}.sto_ss_layer3.txt)\n",
    "'''\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PLPROC\n",
    "run.run_process(\n",
    "    'plproc',\n",
    "    path=preproc_ws,\n",
    "    commands=['plproc.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out kriging factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_in_file(path, input_file, original_str, replacement_str):\n",
    "    # Read in the file\n",
    "    with open(os.path.join(path, input_file), 'r') as file :\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace(original_str, replacement_str)\n",
    "\n",
    "    # Write the file out again\n",
    "    with open(os.path.join(path, input_file), 'w') as file:\n",
    "        file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out kriging factors\n",
    "replace_in_file(preproc_ws, 'plproc.dat', 'calc_kriging_factors', '#calc_kriging_factors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PLPROC\n",
    "run.run_process(\n",
    "    'plproc',\n",
    "    path=preproc_ws,\n",
    "    commands=['plproc.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write template file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "pp_coarse3d_tpl = copy.deepcopy(pp_coarse3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d_tpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d_tpl['kh'] = [f'$kh{r.layer}pp{i + 1:03d}$' for i,r in pp_coarse3d_tpl.iterrows()]\n",
    "pp_coarse3d_tpl['kv'] = [f'$kv{r.layer}pp{i + 1:03d}$' for i,r in pp_coarse3d_tpl.iterrows()] \n",
    "pp_coarse3d_tpl['sy'] = [f'$sy{r.layer}pp{i + 1:03d}$' for i,r in pp_coarse3d_tpl.iterrows()]\n",
    "pp_coarse3d_tpl['ss'] = [f'$ss{r.layer}pp{i + 1:03d}$' for i,r in pp_coarse3d_tpl.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d_tpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_coarse3d_tpl.to_csv(os.path.join(template_ws, 'pp3d_coarse.tpl'), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_prepender(filename, line):\n",
    "    with open(filename, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(line.rstrip('\\r\\n') + '\\n' + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_prepender(os.path.join(template_ws, 'pp3d_coarse.tpl'), 'ptf $')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameterization for SFR\n",
    "Setup pilot points for linear boundary conditions General Head Boundaries (GHBs) and Creek rbcken (SFR), starting with SFR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segfile = pd.read_csv(os.path.join(preproc_ws, 'sfr_segfile.dat'), sep='\\t', names=['x', 'y', 'seg'])\n",
    "display(segfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the segments. This should probably be done in a better way and perhaps changed after the first round of history matching to get a better representation of stream reaches. This solution is just to make things work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_divisor(n):\n",
    "    a = 1\n",
    "    for i in range(2, n):\n",
    "        if n % i == 0:\n",
    "            a = i\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nseg = largest_divisor(len(segfile))\n",
    "#nseg = largest_divisor(nseg) # divide again to reduce the amount of pps from 135 to 45 (perhaps a stupid move)\n",
    "print(nseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_per_seg = int(len(segfile) / nseg)\n",
    "print(rows_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nseg):\n",
    "    segfile.loc[(i * rows_per_seg):((i + 1) * rows_per_seg), ('seg')] = f's{i + 1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite seglist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segfile.to_csv(\n",
    "    os.path.join(preproc_ws, 'sfr_segfile.dat'),\n",
    "    header=None,\n",
    "    index=False,\n",
    "    sep='\\t',\n",
    "    float_format='%.3f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(os.path.join(preproc_ws, 'plproc_seglist_temp.dat'), [\n",
    "f'''\n",
    "### Read model grid ###\n",
    "\n",
    "cl_mf = read_mf6_grid_specs(  &\n",
    "    file=../model/{ml_name}.disv.grb,  &\n",
    "    dimensions=3,             & # Note 3D in this case\n",
    "    slist_layernum = layer,   &\n",
    "    slist_idomain = idomain   &\n",
    "    )\n",
    "\n",
    "\n",
    "### Read Creek rbcken SFR seglist file ###\n",
    "sl_sfr = read_segfile(file=\"sfr_segfile.dat\", protocol=table)\n",
    "\n",
    "### Create clist with sl_sfr as its base ###\n",
    "cl_sfr_pp = create_clist_from_seglist(seglist=sl_sfr, linkage_type=endpoints, dist_thresh=5.0)\n",
    "\n",
    "### Write reports (pps will be constructed based on theses reports) ###\n",
    "cl_sfr_pp.report_dependent_lists(file='report_sfr_seglist.dat')\n",
    "'''\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PLPROC\n",
    "run.run_process(\n",
    "    'plproc',\n",
    "    path=preproc_ws,\n",
    "    commands=['plproc_seglist_temp.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps = pd.read_csv(os.path.join(preproc_ws, 'report_sfr_seglist.dat'), skiprows=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps.columns = sfr_pps.columns.str.replace(r'\\ (?= *?\\ )', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps.iloc[:,0] = sfr_pps.iloc[:,0].str.replace(r'\\ (?= *?\\ )', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps[['Index', 'Int_ID', 'X-coord', 'Y-coord']] = sfr_pps['Index Int_ID X-coord Y-coord'].str.split(' ', 3, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps['cond'] = 86.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps.rename(columns={'Int_ID': 'ppid', 'X-coord': 'x', 'Y-coord': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps = sfr_pps[['ppid', 'x', 'y', 'cond']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sfr_pps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps.to_csv(\n",
    "    os.path.join(preproc_ws, 'sfrpp.dat'),\n",
    "    index=False,\n",
    "    sep='\\t',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameterize the INFLOW parameter of reach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pkgdatafile = f'{ml_name}.sfr_packagedata.txt'\n",
    "shutil.copyfile(os.path.join(model_ws, sfr_pkgdatafile), os.path.join(preproc_ws, sfr_pkgdatafile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(\n",
    "    os.path.join(preproc_ws, f'{ml_name}.sfr_packagedata.txt'),\n",
    "    os.path.join(preproc_ws, f'{ml_name}.sfr_packagedata.tpl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_perdatafile = f'{ml_name}.sfr_perioddata_1.txt'\n",
    "shutil.copyfile(os.path.join(model_ws, sfr_perdatafile), os.path.join(template_ws, sfr_perdatafile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(\n",
    "    os.path.join(template_ws, f'{ml_name}.sfr_perioddata_1.txt'),\n",
    "    os.path.join(template_ws, f'{ml_name}.sfr_perioddata_1.tpl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_prepender(os.path.join(template_ws, f'{ml_name}.sfr_perioddata_1.tpl'), 'ptf $')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws, f'{ml_name}.sfr_perioddata_1.tpl'), 'r') as file:\n",
    "    sfr_perdata = file.readlines() #pestfile partial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_row = sfr_perdata[2].split()\n",
    "display(temp_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_row[-1] = '$sfr_inflow1$\\n'\n",
    "display(temp_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = ' '.join(temp_row)\n",
    "display(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_perdata[2] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws, f'{ml_name}.sfr_perioddata_1.tpl'), 'w') as file:\n",
    "    for line in sfr_perdata:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(os.path.join(preproc_ws, 'plproc_sfr.dat'), [\n",
    "f'''\n",
    "### Read model grid ###\n",
    "\n",
    "cl_mf = read_mf6_grid_specs(  &\n",
    "    file=../model/{ml_name}.disv.grb,  &\n",
    "    dimensions=3,             & # Note 3D in this case\n",
    "    slist_layernum = layer,   &\n",
    "    slist_idomain = idomain   &\n",
    "    )\n",
    "\n",
    "\n",
    "### Read Creek rbcken SFR seglist file ###\n",
    "sl_sfr = read_segfile(file=\"sfr_segfile.dat\", protocol=table)\n",
    "\n",
    "### Create clist with sl_sfr as its base ###\n",
    "cl_sfr_pp = create_clist_from_seglist(seglist=sl_sfr, linkage_type=endpoints, dist_thresh=5.0)\n",
    "\n",
    "### Instruct PLPROC to read the *_cond.dat list file to obtain conductance values at  ###\n",
    "### pilot points by inserting the following function into the PLPROC script.          ###\n",
    "read_list_file(reference_clist='cl_sfr_pp', file='sfrpp.dat', skiprows=1, plist='pp_sfr_cond';column=4)\n",
    "\n",
    "### Instruct PLPROC to build an SLIST of model drain cells to which interpolation must take place ###\n",
    "sfr_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.sfr_packagedata.txt, model_type=mf6_disv, &\n",
    "    list_col_start=2, keytext_start='top_of_file', keytext_end='end_of_file')\n",
    "\n",
    "### Calculate interpolation factors to these model cells through linear interpolation ###\n",
    "calc_linear_interp_factors(source_clist=cl_sfr_pp, target_clist=cl_mf;select=(sfr_cells.ne.0), file=\"factors_sfr_cells.dat\", search_radius=50)\n",
    "\n",
    "### Write ###\n",
    "sfr_cond=new_plist(reference_clist=cl_mf,value=0.0)\n",
    "\n",
    "sfr_cond=pp_sfr_cond.interp_using_file(file=factors_sfr_cells.dat, transform=log)\n",
    "\n",
    "replace_cells_in_lists(                                 &\n",
    "    old_file={ml_name}.sfr_packagedata.tpl,             &\n",
    "    new_file=../model/{ml_name}.sfr_packagedata.txt,    &\n",
    "    model_type=mf6_disv,                                &\n",
    "    list_col_start=2,                                   &\n",
    "    keytext_start='top_of_file',                        &\n",
    "    keytext_end='bottom_of_file',                       &\n",
    "    plist=sfr_cond;column=9;action='replace'            &\n",
    "    )\n",
    "\n",
    "# -- This is for our own interest.\n",
    "cl_mf.report_dependent_lists(file='report-sfr.dat')\n",
    "\n",
    "'''\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PLPROC\n",
    "run.run_process(\n",
    "    'plproc',\n",
    "    path=preproc_ws,\n",
    "    commands=['plproc_sfr.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out kriging factors\n",
    "replace_in_file(preproc_ws, 'plproc_sfr.dat', 'calc_linear_interp', '#calc_linear_interp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct template file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps_tpl = copy.deepcopy(sfr_pps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps_tpl['cond'] = ['$sfrpp{:03d}$'.format(i) for i in range(1, len(sfr_pps_tpl) + 1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps_tpl.to_csv(os.path.join(template_ws, 'sfrpp.tpl'), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_prepender(os.path.join(template_ws, 'sfrpp.tpl'), 'ptf $')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup SFR covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps['zone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps = sfr_pps[['ppid', 'x', 'y', 'zone', 'cond']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_pps.to_csv( # Essentially a copy of sfrpp.dat but without column headers\n",
    "    os.path.join(preproc_ws, 'mksfrpp.dat'),\n",
    "    index=False,\n",
    "    header=False,\n",
    "    sep='\\t',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run MKPPSTAT\n",
    "run.run_process(\n",
    "    'mkppstat',\n",
    "    path=preproc_ws,\n",
    "    commands=['mksfrpp.dat', '10', '1.5', 'sfrppstat.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PPCOV_SVA - pilot point covariance - spatially varying anisotropy\n",
    "run.run_process(\n",
    "    'ppcov_sva',\n",
    "    path=preproc_ws,\n",
    "    commands=['sfrppstat.dat', 'y', 0, 'x',  'cov_sfr.mat', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covmat_sfr = pyemu.Cov.from_ascii(os.path.join(preproc_ws, \"cov_sfr.mat\"))\n",
    "plt.figure(figsize=(8, 8), dpi=100)\n",
    "plt.imshow(covmat_sfr.as_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize SFR pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "plt.ticklabel_format(axis='both', style='plain', useOffset=False) #Show coordinates\n",
    "ax.set_title('SFR pilot point locations', fontsize=18)\n",
    "\n",
    "mapview = flopy.plot.PlotMapView(gwf, layer=0)\n",
    "linecollection = mapview.plot_grid(lw=0.25)\n",
    "\n",
    "plt.scatter([float(i) for i in sfr_pps.x.values], [float(i) for i in sfr_pps.y.values], label='SFR PP')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GHB using pilot points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the file and make an \"template\" of an `old_file` (is this really a template in a PEST context?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb_file = f'{ml_name}.ghb_stress_period_data_1.txt'\n",
    "shutil.copyfile(os.path.join(model_ws, ghb_file), os.path.join(preproc_ws, ghb_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(\n",
    "    os.path.join(preproc_ws, f'{ml_name}.ghb_stress_period_data_1.txt'),\n",
    "    os.path.join(preproc_ws, f'{ml_name}.ghb_stress_period_data_1.tpl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(os.path.join(preproc_ws, 'plproc_ghb.dat'), [\n",
    "f'''\n",
    "# -- We read the MODFLOW 6 binary grid file. We read it as a 2D file.\n",
    "\n",
    "cl_mf = read_mf6_grid_specs(file=../model/{ml_name}.disv.grb, &\n",
    "                              dimensions=3,             &\n",
    "                              slist_layernum = layer,   &\n",
    "                              slist_idomain  = idomain)\n",
    "\n",
    "# -- We now build an SLIST of drain cells to which we want interpolation to take place.\n",
    "\n",
    "ghb_red_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.ghb_stress_period_data_1.txt, model_type=mf6_disv, &\n",
    "    list_col_start=1, keytext_start='top_of_file', keytext_end='2387  ghb_orange')\n",
    "    \n",
    "ghb_orange_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.ghb_stress_period_data_1.txt, model_type=mf6_disv, &\n",
    "    list_col_start=1, keytext_start='2387  ghb_red', keytext_end='2792  ghb_yellow')\n",
    "\n",
    "ghb_yellow_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.ghb_stress_period_data_1.txt, model_type=mf6_disv, &\n",
    "    list_col_start=1, keytext_start='2800  ghb_orange', keytext_end='2802  ghb_limegreen')\n",
    "\n",
    "ghb_limegreen_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.ghb_stress_period_data_1.txt, model_type=mf6_disv, &\n",
    "    list_col_start=1, keytext_start='3090  ghb_yellow', keytext_end='1  ghb_royalblue')\n",
    "\n",
    "ghb_royalblue_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.ghb_stress_period_data_1.txt, model_type=mf6_disv, &\n",
    "    list_col_start=1, keytext_start='3057  ghb_limegreen', keytext_end='1  ghb_blueviolet')\n",
    "\n",
    "ghb_blueviolet_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.ghb_stress_period_data_1.txt, model_type=mf6_disv, &\n",
    "    list_col_start=1, keytext_start='2802  ghb_royalblue', keytext_end='316  ghb_magenta')\n",
    "\n",
    "ghb_magenta_cells = cl_mf.find_cells_in_lists(file=../model/{ml_name}.ghb_stress_period_data_1.txt, model_type=mf6_disv, &\n",
    "    list_col_start=1, keytext_start='316  ghb_blueviolet', keytext_end='end_of_file')\n",
    "\n",
    "# -- We next read the pilot points file and pilot point locations and values.\n",
    "\n",
    "cl_pp = read_list_file(file='ghbpp.dat',                                          &\n",
    "                       id_type='character',                                       &\n",
    "                       dimensions=2,                                              &\n",
    "                       skiprows=1,                                                &\n",
    "                       plist='ghbcond_pp';column=4)\n",
    "\n",
    "# -- Initialize the drain conductivity PLIST\n",
    "ghbcond_mf=new_plist(reference_clist=cl_mf,value=0.0)\n",
    "\n",
    "\n",
    "# -- We now interpolate from pilot points to the drain conductances.\n",
    "#    We use inverse power of distance; but any interpolation method could be used.\n",
    "\n",
    "ghbcond_mf(select=((ghb_red_cells.ne.0)&&(layer.eq.1)))=ghbcond_pp.ivd_interpolate_2d( &\n",
    "                                        transform='log',                          &\n",
    "                                        inv_power=2.0,                            &\n",
    "                                        min_points=2,                             &\n",
    "                                        max_points=20,                            &\n",
    "                                        search_radius=1.0e20) # Perhaps too large?\n",
    "\n",
    "ghbcond_mf(select=((ghb_orange_cells.ne.0)&&(layer.eq.1)))=ghbcond_pp.ivd_interpolate_2d( &\n",
    "                                        transform='log',                          &\n",
    "                                        inv_power=2.0,                            &\n",
    "                                        min_points=2,                             &\n",
    "                                        max_points=20,                            &\n",
    "                                        search_radius=1.0e20)\n",
    "\n",
    "ghbcond_mf(select=((ghb_yellow_cells.ne.0)&&(layer.eq.1)))=ghbcond_pp.ivd_interpolate_2d( &\n",
    "                                        transform='log',                          &\n",
    "                                        inv_power=2.0,                            &\n",
    "                                        min_points=2,                             &\n",
    "                                        max_points=20,                            &\n",
    "                                        search_radius=1.0e20)\n",
    "\n",
    "ghbcond_mf(select=((ghb_limegreen_cells.ne.0)&&(layer.eq.1)))=ghbcond_pp.ivd_interpolate_2d( &\n",
    "                                        transform='log',                          &\n",
    "                                        inv_power=2.0,                            &\n",
    "                                        min_points=2,                             &\n",
    "                                        max_points=20,                            &\n",
    "                                        search_radius=1.0e20)\n",
    "\n",
    "ghbcond_mf(select=((ghb_royalblue_cells.ne.0)&&(layer.eq.1)))=ghbcond_pp.ivd_interpolate_2d( &\n",
    "                                        transform='log',                          &\n",
    "                                        inv_power=2.0,                            &\n",
    "                                        min_points=2,                             &\n",
    "                                        max_points=20,                            &\n",
    "                                        search_radius=1.0e20)\n",
    "\n",
    "ghbcond_mf(select=((ghb_blueviolet_cells.ne.0)&&(layer.eq.1)))=ghbcond_pp.ivd_interpolate_2d( &\n",
    "                                        transform='log',                          &\n",
    "                                        inv_power=2.0,                            &\n",
    "                                        min_points=2,                             &\n",
    "                                        max_points=20,                            &\n",
    "                                        search_radius=1.0e20)\n",
    "\n",
    "ghbcond_mf(select=((ghb_magenta_cells.ne.0)&&(layer.eq.1)))=ghbcond_pp.ivd_interpolate_2d( &\n",
    "                                        transform='log',                          &\n",
    "                                        inv_power=2.0,                            &\n",
    "                                        min_points=2,                             &\n",
    "                                        max_points=20,                            &\n",
    "                                        search_radius=1.0e20)\n",
    "\n",
    "# -- A new model input file is written.\n",
    "\n",
    "replace_cells_in_lists(old_file={ml_name}.ghb_stress_period_data_1.tpl,               &\n",
    "                       new_file=../model/{ml_name}.ghb_stress_period_data_1.txt,      &\n",
    "                       model_type=mf6_disv,                                       &\n",
    "                       list_col_start=1,                                          &\n",
    "                       keytext_start='top_of_file',                               &\n",
    "                       keytext_end='bottom_of_file',                              &\n",
    "                       plist=ghbcond_mf;column=4;action='replace')\n",
    "\n",
    "# -- This is for our own interest.\n",
    "#cl_mf6.report_dependent_lists(file='report-ghb.dat')\n",
    "'''\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PLPROC\n",
    "run.run_process(\n",
    "    'plproc',\n",
    "    path=preproc_ws,\n",
    "    commands=['plproc_ghb.dat']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GHB covmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbpp_df = pd.read_csv(os.path.join(preproc_ws, 'ghbpp.dat'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbpp_df['zone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghbpp_df = ghbpp_df[['ppid','x', 'y', 'zone', 'cond', 'bnames']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one pp derivative of each ghb boundname\n",
    "for bname in ghbpp_df.bnames.unique():\n",
    "    ghbpp_df.loc[ghbpp_df['bnames'] == bname].to_csv(\n",
    "        os.path.join(preproc_ws, f'mkghb{bname}pp.dat'),\n",
    "        index=False,\n",
    "        header=False,\n",
    "        sep='\\t',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(preproc_ws):\n",
    "    if 'mkghb' in file:\n",
    "        run.run_process(\n",
    "            'mkppstat',\n",
    "            path=preproc_ws,\n",
    "            commands=[file, '10', '1.5', f'{file[2:-4]}_stat.dat']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(preproc_ws):\n",
    "    if 'ghb' in file and '_stat.dat' in file:\n",
    "        filename = f'cov_{file[:6]}.mat'\n",
    "        run.run_process(\n",
    "            'ppcov_sva',\n",
    "            path=preproc_ws,\n",
    "            commands=[file, 'y', 1, 'x',  filename, '']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize GHB pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "plt.ticklabel_format(axis='both', style='plain', useOffset=False) #Show coordinates\n",
    "ax.set_title('GHB pilot point locations', fontsize=18)\n",
    "\n",
    "mapview = flopy.plot.PlotMapView(gwf, layer=0)\n",
    "linecollection = mapview.plot_grid(lw=0.25)\n",
    "\n",
    "for bname in ghbpp_df['bnames'].unique():\n",
    "    df = ghbpp_df.loc[ghbpp_df['bnames'] == bname]\n",
    "    x = [float(i) for i in df.x.values]\n",
    "    y = [float(i) for i in df.y.values]\n",
    "    \n",
    "    plt.scatter(x, y, color=bname, label=f'ghb_{bname}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterize LUMPREM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to be adjusted through history-matching are:\n",
    "\n",
    "\n",
    "| parameter | default | initial | upper bound | lower bound |\n",
    "| --------- |--------:|--------:|-------------| ------------|\n",
    "| maxvol    | 0.5     | 0.2     | 1           | 0.01        |\n",
    "| rdelay    | 5       | 5       | 10          | 1           |\n",
    "| mdelay    | 1       | 1       | 5           | 0           |\n",
    "| Ks        | 0.1     | 86.4    | 864.0       | 0.000864    |\n",
    "| m         | 0.5     | 1.0     | 2.0         | 0.01        |\n",
    "| l         | 0.5     | 0.5     | 1.0         | 0.0         |\n",
    "| mflowmax  | 0.1     | 0.1     | 0.2         | 0.01        |\n",
    "| f         | 0.5     | 0.5     | 0.9         | 0.01        |\n",
    "|          | 1.0     | 1.0     | 10.0        | 0.01        |\n",
    "\n",
    "- *maxvol* The volume of the soil moisture store.\n",
    "- *rdelay* Delay, in days, between water draining from the soil moisture store.\n",
    "- *mdelay* Delay, in days, between water leaving the soil moisture store.\n",
    "- *Ks* Hydraulic conductivity (meters per day). Initial: 86.4 (0.001 m/s)\n",
    "- *m* A parameter determining the shape of the drainage rate vs. stored water relationship.\n",
    "- *l* A pore-connectivity parameter (estimated by Mualem, 1976, to be about 0.5 for many soils).\n",
    "- *mflowmax* Maximum macropore recharge allowed per day.\n",
    "- *f* Crop factor.\n",
    "- ** A parameter determining the shape of the evaporation rate vs. stored water relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameterize_lr_model(lr_template_file, lr_ml_name):\n",
    "    # Read in the file\n",
    "    with open(os.path.join(template_ws, lr_template_file), 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Replace the target string\n",
    "    lines[2] = f'$maxvol_{lr_ml_name}$  0 \\n' #earth properties maxvolume & irrigvolfrac\n",
    "    lines[3] = f'$rdelay_{lr_ml_name}$  $mdelay_{lr_ml_name}$ \\n' #earth properties delay factors\n",
    "    lines[4] = f'$ks_{lr_ml_name}$  $m_{lr_ml_name}$  $l_{lr_ml_name}$  $mfmax_{lr_ml_name}$  \\n' #earth properties factors\n",
    "    lines[24] = f'$crfac_{lr_ml_name}$  $gamma_{lr_ml_name}$  \\n' #crop factor & gamma\n",
    "\n",
    "    # Write the file out again\n",
    "    with open(os.path.join(template_ws, lr_template_file), 'w') as file:\n",
    "        for line in lines:\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameterize recharge model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterize_lr_model('lr_lu1.tpl', 'lu1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameterize GHB models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(template_ws):\n",
    "    if 'lr_' in file and 'lu1' not in file:\n",
    "        parameterize_lr_model(file, file[3:][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build partial PEST control file using TPL2PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write tpl2pst script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(os.path.join(template_ws, 'tpl2pst.in'), [\n",
    "f'''\n",
    "# An example TPL2PST input file\n",
    "\n",
    "start template_file\n",
    "  template_file     = pp3d_coarse.tpl\n",
    "  model_input_file  = pp3d_coarse.dat\n",
    "  transform         = log\n",
    "  value             = 1.0\n",
    "  lower_bound       = 0.1     \n",
    "  upper_bound       = 10\n",
    "  pargroup          = changethis\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = sfrpp.tpl\n",
    "  model_input_file  = sfrpp.dat\n",
    "  transform         = log\n",
    "  value             = 86.4\n",
    "  lower_bound       = 0.000864     \n",
    "  upper_bound       = 864.0\n",
    "  pargroup          = sfrc\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = ghbpp.tpl\n",
    "  model_input_file  = ghbpp.dat\n",
    "  transform         = log\n",
    "  value             = 86.4\n",
    "  lower_bound       = 0.000864     \n",
    "  upper_bound       = 864.0\n",
    "  pargroup          = ghbc\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = hagfors_1.sfr_perioddata_1.tpl\n",
    "  model_input_file  = hagfors_1.sfr_perioddata_1.txt\n",
    "  transform         = log\n",
    "  value             = 15000\n",
    "  lower_bound       = 100     \n",
    "  upper_bound       = 40000\n",
    "  pargroup          = sfrinfl1\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_lu1.tpl \n",
    "  model_input_file  = lr_lu1.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = rch\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_red.tpl \n",
    "  model_input_file  = lr_red.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = lrghbred\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_orange.tpl \n",
    "  model_input_file  = lr_orange.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = lrghbora\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_yellow.tpl \n",
    "  model_input_file  = lr_yellow.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = lrghbyel\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_limegreen.tpl \n",
    "  model_input_file  = lr_limegreen.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = lrghblim\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_royalblue.tpl \n",
    "  model_input_file  = lr_royalblue.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = lrghbroy\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_blueviolet.tpl \n",
    "  model_input_file  = lr_blueviolet.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = lrghbblu\n",
    "end template_file\n",
    "\n",
    "start template_file\n",
    "  template_file     = lr_magenta.tpl \n",
    "  model_input_file  = lr_magenta.in\n",
    "  transform         = log\n",
    "  value             = 1 # change this\n",
    "  lower_bound       = 0.1 # change this     \n",
    "  upper_bound       = 10 # change this\n",
    "  pargroup          = lrghbmag\n",
    "end template_file\n",
    "'''\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run TPL2PST\n",
    "run.run_process(\n",
    "    'tpl2pst',\n",
    "    path=template_ws,\n",
    "    commands=['tpl2pst.in', 'param.pst']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running tpl2pst doesn't work for some reason. has to be run subprocess below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "p = subprocess.run(['tpl2pst', 'tpl2pst.in', 'param.pst'], cwd=template_ws, stdout=subprocess.PIPE, text=True)\n",
    "for row in p.stdout.split('\\n'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(template_ws) # param.pst is now created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make necessary changes to param.pst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws, 'param.pst'), 'r') as file:\n",
    "    pstfilep_param = file.readlines() #pestfile partial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Relevant indices:') # print relevant indices of the pstfilep_param\n",
    "print(pstfilep_param.index('* parameter groups\\n'))\n",
    "print(pstfilep_param.index('* parameter data\\n'))\n",
    "print(pstfilep_param.index('* observation groups\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change parameter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by changing all the parameters marked with \"changethis\" (as suggested by calibration tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramdata_start = pstfilep_param.index('* parameter data\\n') + 1\n",
    "paramdata_stop = pstfilep_param.index('* observation groups\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_df = pd.DataFrame(pstfilep_param[paramdata_start:paramdata_stop]) # read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_df.iloc[:,0] = pardata_df.iloc[:,0].str.replace(r'\\ (?= *?\\ )', '') # clean up data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_cols = ['PARNME', 'PARTRANS', 'PARCHGLIM', 'PARVAL1', 'PARLBND', 'PARUBND', 'PARGP', 'SCALE', 'OFFSET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_df[pardata_cols] = pardata_df[0].str.split(' ', 9, expand=True) # add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_df = pardata_df[pardata_cols] # remove redundant column\n",
    "display(pardata_df) # view df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the dataframe based on parameters (as suggested by calibration tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_df = pardata_df.sort_values(['PARNME', 'PARGP']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign new parameter groups (PARGP) for each type of parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnme_pargrp = [\n",
    "    'ghbcblu', 'ghbclim', 'ghbcmag', 'ghbcora', 'ghbcred', \n",
    "    'ghbcroy','ghbcyel', 'kh1pp', 'kh2pp', 'kh3pp', 'kv1pp',\n",
    "    'kv2pp', 'kv3pp', 'sfrpp', 'ss1pp', 'ss2pp', 'ss3pp',\n",
    "    'sy1pp', 'sy2pp', 'sy3pp'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in parnme_pargrp:\n",
    "    pardata_df.loc[pardata_df['PARNME'].str.contains(i), 'PARGP'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign new (initial) parameter value, lower and upper bounds for the non-linear pilot point parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parval1 = {'kh': 86.4, 'kv': 8.64, 'sy': 0.2, 'ss': 0.000001}\n",
    "parlbnd = {'kh': 8.64E-04, 'kv': 8.64E-05, 'sy': 0.05, 'ss': 0.00000001}\n",
    "parubnd = {'kh': 864.0, 'kv': 864.0, 'sy': 0.35, 'ss': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, col in zip([parval1, parlbnd, parubnd], ['PARVAL1', 'PARLBND', 'PARUBND']):\n",
    "    for k,v in d.items():\n",
    "        pardata_df.loc[pardata_df['PARGP'].str.contains(k), col] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pardata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign new (initial) parameter value, lower and upper bounds for the **lumprem** parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from df to list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_list = pardata_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pardata_new = []\n",
    "for i in pardata_list:\n",
    "    pardata_new.append(' '.join(str(v) for v in i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the old parameter data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pstfilep_param[paramdata_start:paramdata_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will now be replaced by the new parameter data, which is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(pardata_new) # show list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a temporary copy of the original `pstfilep_param`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_param_temp = pstfilep_param[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the old parameter data with the changed pardata in the templist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index, replacement) in zip([i for i in range(paramdata_start, paramdata_stop)], pardata_new):\n",
    "    pstfilep_param_temp[index] = replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter data has now been replaced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pstfilep_param_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change parameter groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevant indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pargrp_start = pstfilep_param_temp.index('* parameter groups\\n') + 1\n",
    "pargrp_stop = pstfilep_param_temp.index('* parameter data\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the current parameter grous config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pargrp_df = pd.DataFrame(pstfilep_param_temp[pargrp_start:pargrp_stop])\n",
    "pargrp_df.iloc[:,0] = pargrp_df.iloc[:,0].str.replace(r'\\ (?= *?\\ )', '')\n",
    "pargrp_cols = ['PARGPNME', 'INCTYP', 'DERINC', 'DERINCLB', 'FORCEN', 'DERINCMUL', 'DERMTHD']\n",
    "pargrp_df[pargrp_cols] = pargrp_df[0].str.split(' ', 7, expand=True)\n",
    "pargrp_df = pargrp_df[pargrp_cols]\n",
    "display(pargrp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate this data structure (no need to use a dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pargrp_new = [' '.join(str(v) for v in [i, 'relative', 0.015, 0.0, 'switch', 2, 'parabolic\\n']) for i in pardata_df['PARGP'].unique()]\n",
    "display(pargrp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new copy of the partial pestfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_param_new = pstfilep_param_temp[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the old parameter groups with the changed pargp in the temp2list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_param_new[pargrp_start:pargrp_stop] = pargrp_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter groups have now been changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pstfilep_param_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change NPARGP (number of parameter groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the third column of the third index (number 3 below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_param_new[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_row = pstfilep_param_new[3].split()\n",
    "display(temp_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npargp = len(pardata_df['PARGP'].unique()) # number of parameter groups\n",
    "print(npargp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_row[2] = str(npargp)\n",
    "temp_row[-1] = '0\\n'\n",
    "display(temp_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = ' '.join(temp_row)\n",
    "display(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_param_new[3] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pstfilep_param_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write new partial pest parameter file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws, 'partial_param.pst'), 'w') as file:\n",
    "    for line in pstfilep_param_new:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy pestfile to pestdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_pestfile = 'partial_param.pst'\n",
    "shutil.copyfile(os.path.join(template_ws, partial_pestfile), os.path.join(pest_ws, partial_pestfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(\n",
    "    os.path.join(pest_ws, 'partial_param.pst'),\n",
    "    os.path.join(pest_ws, 'calib0.pst')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix model input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pest_ws, 'calib0.pst'), 'r') as file:\n",
    "    pstfilep_calib = file.readlines() #pestfile partial parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_io_start = pstfilep_calib.index('* model input/output\\n') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_calib[ml_io_start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepend1, prepend2 = '.\\\\template\\\\', '..\\\\runmodel\\\\preproc\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ml_io = []\n",
    "for i in pstfilep_calib[ml_io_start:]:\n",
    "    for string, j in zip([prepend1, prepend2], i.split()):\n",
    "        new_ml_io.append(string + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ml_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ml_io = [' '.join(i) for i in zip(new_ml_io[0::2], new_ml_io[1::2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ml_io = [i+'\\n' for i in new_ml_io]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ml_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstfilep_calib[ml_io_start:] = new_ml_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite `calib0.pst`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pest_ws, 'calib0.pst'), 'w') as file:\n",
    "    for line in pstfilep_calib:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas_lowpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters: kh, kx, sy, ss, sfrcond, ghbcond, lu1 lumprem recharge model (still tbd ghb lumprem models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
